# log

## ubuntu
- reading book about dl from C1-5
  - master the basic maths theory of 神经网络
    - 主要重新掌握了梯度下降的理论
  -激活函数
    - 每个激活函数就像神经突触上的激活一样，到达一定阈值就会兴奋，并且传递兴奋
    - 激活函数一般与交叉熵一起出现，激活函数是前向传播非线性变换的关键，交叉熵是计算损失函数的基本算法，二者在相同模型下运用的类型相似。
  - 批处理改变第一层的维度
  - 交叉熵负责分类，均方差负责回归
    - 交叉熵基于概率和信息论，能更好地衡量分类准确性
    - 均方差直接度量连续值的预测误差，优化更简单有效
- 吴恩达视频
  - 数据预处理时归一化：为了方便在梯度下降时lr调整，**预防不同特征数据差距过大导致步长难以控制**
  - 见（5：54）*https://www.bilibili.com/video/BV1Bq421A74G?spm_id_from=333.788.videopod.episodes&vd_source=070587237b855803f3b0c7826b7e0049&p=25*
  - **解决过拟合**
    - 加训练数据量 collect more data 
    - 使用较少特征 降低复杂度 select feature
    - 正则化 eliminate feature，通常不对b正则化而是w regularization
      - 首先不一定有用
      - 其次是真没有用，因为b充其量就是对函数的位置的变化，可能有用但极其有限
    - 